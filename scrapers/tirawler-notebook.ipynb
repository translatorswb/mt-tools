{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tigrinya web crawler (Tirawler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect page links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import requests\n",
    "import os\n",
    "import lazynlp\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def url_exists(url):\n",
    "    request = requests.get(url)\n",
    "    return request.status_code == 200\n",
    "\n",
    "def download_url(url, path):\n",
    "    if not os.path.exists(path):\n",
    "        if not url_exists(url):\n",
    "            print(\"Url doesn't exist:\", url)\n",
    "            return 'END'\n",
    "        \n",
    "        print(\"Downloading\", url)\n",
    "        code, page_content = lazynlp.download_page(url)\n",
    "        with open(path, 'wb') as f:\n",
    "            f.write(page_content)\n",
    "    else:\n",
    "        with open(path, 'r') as f:\n",
    "            page_content = f.read()\n",
    "    return page_content\n",
    "\n",
    "def parse_topic_headlines(page_content, base_url):\n",
    "    soup = BeautifulSoup(page_content)\n",
    "    \n",
    "    media_block = soup.find(\"div\", {\"class\": \"media-block-wrap\"})\n",
    "    article_elems = media_block.findAll(\"a\", {\"class\":\"img-wrap\"})\n",
    "    \n",
    "    article_links = [base_url + e.attrs[\"href\"] for e in article_elems]\n",
    "    \n",
    "    return article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = mediablock.findAll(\"a\", {\"class\":\"img-wrap\"})\n",
    "firstlink = links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) VOA News Tigrigna https://tigrigna.voanews.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Url doesn't exist: https://tigrigna.voanews.com/z/5444?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/2916?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/3329?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/3316?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/2918?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/2923?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/2917?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/3325?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/2920?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/4270?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/4611?p=101\n",
      "Url doesn't exist: https://tigrigna.voanews.com/z/4457?p=101\n"
     ]
    }
   ],
   "source": [
    "#Download directory\n",
    "voa_downdir = \"urls/voanews\"\n",
    "topic_urls_downdir = os.path.join(voa_downdir, 'topic-headlines')\n",
    "if not os.path.exists(voa_downdir):\n",
    "    os.makedirs(voa_downdir)\n",
    "\n",
    "base_url = 'https://tigrigna.voanews.com'\n",
    "topic_base_urls = {'africa1':'https://tigrigna.voanews.com/z/5444', \n",
    "              'africa2':'https://tigrigna.voanews.com/z/2916', \n",
    "              'world':'https://tigrigna.voanews.com/z/3329', \n",
    "              'politics': 'https://tigrigna.voanews.com/z/3316',\n",
    "              'health': 'https://tigrigna.voanews.com/z/2918',\n",
    "              'youth': 'https://tigrigna.voanews.com/z/2923',\n",
    "              'america': 'https://tigrigna.voanews.com/z/2917',\n",
    "              'culture': 'https://tigrigna.voanews.com/z/3325',\n",
    "              'people': 'https://tigrigna.voanews.com/z/2920',\n",
    "              'kenya': 'https://tigrigna.voanews.com/z/4270',\n",
    "              'UN': 'https://tigrigna.voanews.com/z/4611',\n",
    "              '2015': 'https://tigrigna.voanews.com/z/4457'\n",
    "              }\n",
    "\n",
    "topic_article_urls = {topic:[] for topic in topic_base_urls.keys()}\n",
    "\n",
    "#Download and process all\n",
    "    \n",
    "for topic_key in topic_base_urls:\n",
    "    topic_dir = os.path.join(topic_urls_downdir,topic_key)\n",
    "    if not os.path.exists(topic_dir):\n",
    "        os.makedirs(topic_dir)\n",
    "    \n",
    "    topic_base_url = topic_base_urls[topic_key]\n",
    "\n",
    "    page_no = 0\n",
    "    article_list_url = topic_base_url\n",
    "\n",
    "    while(True):\n",
    "        \n",
    "        article_list_html_path = os.path.join(topic_dir, topic_key + \"_\" + str(page_no) + \".html\")\n",
    "\n",
    "        article_list_html_content = download_url(article_list_url, article_list_html_path)\n",
    "\n",
    "        if article_list_html_content == 'END':\n",
    "            break\n",
    "\n",
    "        topic_article_urls[topic_key].extend(parse_topic_headlines(article_list_html_content, base_url))\n",
    "\n",
    "        #get next page\n",
    "        page_no += 1\n",
    "        article_list_url = topic_base_url + '?p=' + str(page_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_article_urls['prefecture2016'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and parse articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(html_content, out_txt_path):\n",
    "    soup = BeautifulSoup(html_content)\n",
    "    text = \"\"\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(\"h1\", {\"class\":\"pg-title\"})\n",
    "        text += title.text\n",
    "#     except:\n",
    "#         print(\"No title\")\n",
    "        \n",
    "    try:\n",
    "        media_block = soup.find(\"div\", {\"class\": \"wsw\"})\n",
    "        text_elems = media_block.findChildren(\"p\", {\"class\":\"\"})\n",
    "        text_elems.extend(media_block.findChildren(\"p\", {\"class\":\"xmsonormal\"}))\n",
    "\n",
    "        text += '\\n'.join([elem.text for elem in text_elems if elem.text and not elem.text.isspace()]) \n",
    "#     except:\n",
    "#         print(\"No text content\")\n",
    "    \n",
    "    if text:\n",
    "        with open(out_txt_path, 'w') as f:\n",
    "            f.write(text)\n",
    "        return text\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ዋሽንግተን: ሱሉሳዊ ልዝብ ሚንስትራት ኢትዮጵያ፤ሱዳንን ግብጺን ቀጺሉ ውዒሉ'ሎ\n",
      "\n",
      "ፕረዚደንት ትራምፕ፡ ትማሊ ኣብዚ ኣብ ዋሽንግተን፡ ምስ ሚንስተራት ጉዳያት ወጻኢ ኢትዮጵያ፡ ግብጽን ሱዳንን ተራኺቡ- ብዛዕባ ምዕባለታት ዝቕጽል ዝሎ ዝርርብ ዓቢ ህዳሰ ግድብ ኢትዮጵያው`ን ተዛሪቡ።\n",
      "ፕረዚደንት ትራምፕ ኣብቲ እዋን፡ ዩናይትድ ስቴትስ ኣብ ምትሕብባር ዝተመስረተ፡ ቀጻልነት ዘለዎን ንኹሎም ዝረብሕን ሓባራዊ ስምምዕ ክግበር ከምትድግፍ`ውን ደጊማ ተረጋግጽ ከም ዝበለ ካብ ዋይት ሃውስ ዝረኸብናዮ ሓበሬታ የመልክት።\n",
      "ፕረዚደንት ትራምፕ ብምትሕሓዝ፡ ስለስቲኤን ሃገራት ነዚ ዕድል`ዚ ተጠቒመን፡ መጻኢ ወሎዶታተን ካብ`ዚ ኣገዳሲ ጸጋታት ማይ ተጠቀምቲ ንክኾኑ ብሓባር ንክሰርሓን ክዓብያን ክርእዩ ዩናይትድ ስቴትስ ድልየታ ምዃኑ ደጊማ ተረጋግጽ ኢሉ።\n",
      "ትማሊ ዝጀመረ 4ይ ዙርያ ዝርርብ ግብጺ፡ ኢትዮጵያን ሱዳንን ሎሚ`ውን ቀጺሉ ውዒሉ`ሎ፡ ዝተበጽሐ ነገር እንተሎ ዛጊድ ዝተፈልጠ ነገር የለን።\n",
      "ብኻልእ ወገን ኣብ`ዚ ሰዓት`ዚ ኣብ`ዚ ኣብ ዋሽንግተን ዲሲ ዝርከብ ኢምባሲ ኢትዮጵያ፡ ብሚንስትር ጉዳያት ወጻኢ ገዱ እንዳርጋቸው ዝምርሑ ልኡኻት ኢትዮጵያ ጋዜጣዊ መግለጺ ይህቡ`ለው። \n"
     ]
    }
   ],
   "source": [
    "#Experiments\n",
    "soup = BeautifulSoup(article_html_content)\n",
    "media_block = soup.find(\"div\", {\"class\": \"wsw\"})\n",
    "text_elems = media_block.findChildren(\"p\", {\"class\":\"\"})\n",
    "text_elems.extend(media_block.findChildren(\"p\", {\"class\":\"xmsonormal\"}))\n",
    "article_text = '\\n'.join([elem.text for elem in text_elems if elem.text and not elem.text.isspace()])    \n",
    "\n",
    "title = soup.find(\"h1\", {\"class\":\"pg-title\"})\n",
    "print(title.text)\n",
    "\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "africa1\n",
      "africa2\n",
      "world\n",
      "politics\n",
      "health\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "youth\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "america\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "culture\n",
      "people\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "kenya\n",
      "UN\n",
      "No title\n",
      "No title\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No title\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "2015\n",
      "No title\n",
      "No text content\n",
      "No title\n",
      "No text content\n",
      "No text content\n",
      "No title\n",
      "No text content\n"
     ]
    }
   ],
   "source": [
    "#Parse all articles\n",
    "articlehtmldir = os.path.join(voa_downdir, 'topic-articles')\n",
    "articletextdir = os.path.join(voa_downdir, 'topic-text')\n",
    "\n",
    "article_text_paths = []\n",
    "\n",
    "for topic in topic_article_urls.keys():\n",
    "    print(topic)\n",
    "    topic_articlehtmldir = os.path.join(articlehtmldir, topic)\n",
    "    if not os.path.exists(topic_articlehtmldir):\n",
    "        os.makedirs(topic_articlehtmldir)\n",
    "        \n",
    "    topic_articletextdir = os.path.join(articletextdir, topic)\n",
    "    if not os.path.exists(topic_articletextdir):\n",
    "        os.makedirs(topic_articletextdir)\n",
    "    \n",
    "    file_counter = 1\n",
    "    for article_url in topic_article_urls[topic]:\n",
    "        #print(article_url)\n",
    "        article_html_path = os.path.join(topic_articlehtmldir, f\"{file_counter:04d}\" + '.html')\n",
    "        article_text_path = os.path.join(topic_articletextdir, f\"{file_counter:04d}\" + '.txt')\n",
    "        \n",
    "        article_html_content = download_url(article_url, article_html_path)\n",
    "        \n",
    "        article_text = parse_article(article_html_content, article_text_path)\n",
    "        \n",
    "        file_counter += 1\n",
    "        if article_text:\n",
    "            article_text_paths.append(article_text_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ending_punkset = ['።', '፨', '፠', '፧', '?']\n",
    "\n",
    "def sentence_tokenize(doc):\n",
    "    \"\"\"\n",
    "    Splits the document into sentences using end punctuation.\n",
    "\n",
    "    :param doc: to split\n",
    "    :return: the list of sentence strings from the document\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    doc = ' '.join(doc.split())\n",
    "\n",
    "    curr_sent = \"\"\n",
    "    for c in doc:\n",
    "        curr_sent += c\n",
    "        if c in sent_ending_punkset:\n",
    "            tokens.append(curr_sent)\n",
    "            curr_sent = \"\"\n",
    "\n",
    "    if curr_sent:\n",
    "        tokens.append(curr_sent)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus_lines = []\n",
    "\n",
    "for path in article_text_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        #article_lines = [line for line in f.read().splitlines()]\n",
    "        lines = [line for line in f.read().splitlines() if line]\n",
    "        text_corpus_lines.extend(lines)\n",
    "    #break\n",
    "    \n",
    "print(\"No. lines\", len(text_corpus_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. sentences 49075\n"
     ]
    }
   ],
   "source": [
    "text_corpus_sentences = []\n",
    "\n",
    "for line in text_corpus_lines:\n",
    "    text_corpus_sentences.extend(sentence_tokenize(line))\n",
    "\n",
    "print(\"No. sentences\", len(text_corpus_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to file\n",
    "output_file = 'corpora/voanews_sentences.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    f.writelines(\"%s\\n\" % sent for sent in text_corpus_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
